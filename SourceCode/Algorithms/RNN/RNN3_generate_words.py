import torch  # for RNN model and generating characters from it.  
import torch.nn as nn  # to define neural networks such as rnn models etc, here we use this for creating our custom RNNmodel
from operator import attrgetter
import numpy as np
import os
from multiprocessing import Pool, cpu_count  # to use multiple processes concurrently and perform the task in parallel.  
# to run process only one time per program execution (ensure that each seed is unique). Use context manager for this purpose: 'with'.   
import random  # used for creating a range of seeds or start characters, can be either uppercase letters, lowercase letters or commonly used symbols.
from string import ascii_uppercase, ascii_lowercase, punctuation  # use to create possible initials (seeds) from different categories: Uppercase letters, Lowercase letters and Special Characters/Symbols.  
import time  # for timing purposes  
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # select cpu or gpu depending on availability (use cuda if available).

# define range of initial seeds: capital letters, small letters and commonly used special symbols.
char_range = ascii_uppercase + ascii_lowercase + punctuation 
rngs = [random.choice(char_range) for _ in range(10)]  # generate a list of random initial seeds or characters  
num_words = 10000 # number of words to be generated per process. It will keep the memory usage under control by not storing more than this amount of data at once.
word_lengths = [8] + [6,7,9,10]# list of lengths for the words to generate in each seed or start character.  
temperatures = [0.5]*len(rngs) # a predefined temperature value which controls randomness during word generation. 
num_processes = cpu_count() # number of processors available, used here to determine how many processes we can run concurrently.  
total_words = num_words*len(word_lengths)*len(temperatures) # total number of words that will be generated when running the program (product of word count per process * length options for each seed/start character * temperature options). 

# load pretrained model and its parameters.  
char_to_idx = torch.load('ML_Algorithms/RNN/Human/char_to_idx.pth') # mapping from characters to their indices in the range [0, len(chars)-1]. Used for creating input tensor for RNN model. 
idx_to_char = torch.load('ML_Algorithms/RNN/Human/idx_to_char.pth') # reverse of char_to_idx: mapping from indices back to their corresponding characters. Used for decoding output probabilities into actual text characters during word generation process. 
chars = sorted(char_to_idx.keys())# all possible unique characters that the RNN model can generate words/sentences from (includes uppercase, lowercase letters and special symbols).  
input_size = len(chars) # size of input layer in our rnn model, equivalent to number of unique elements in character set (len(chars)). 
hidden_size = 64 # size of hidden state for each LSTM cell in the RNN stack. This is a hyperparameter which could be tuned for better performance. Here we use default value: 64.  
num_layers = 2 # number of layers (or depth) in our rnn model, higher values may increase computational cost but possibly improve accuracy. Default value used here: 2.  
model = RNNModel(input_size, hidden_size, num_layers).to(device)# create instance of RNNModel class and move it to the chosen device (cuda or cpu).   
model.load_state_dict(torch.load('ML_Algorithms/RNN/Human/rnn_model.pth')) # load pretrained weights from file into our rnn model object.  
# define helper functions:  generate single word with given parameters and save generated words to txt files in a directory called "output" if it does not exist yet, otherwise append to existing files.
def write_word(char):
    start_char = char # first character of the string we want to generate (starting point for RNN model).  
    word_length = random.choice(word_lengths)# length of the word that needs to be generated by our rnn model. This is a randomly chosen value from predefined options: [8,6,7,9,10]. 
    temperature = temperatures[random.randint(0, len(temperatures)-1)] # random temperature setting for controlling randomness during word generation process.  
    generated_word = generate_word(model=model, start_char=start_char, length=word_length, temperature=temperature)# use RNN model to generate a single word based on given parameters (initial character or seed, desired word length and chosen temperature setting). 
    
    filename = f'output/seed_{ord(start_char)}-length_{word_length}-temp_{temperature}.txt' # construct name for output file where generated words will be saved.  
    os.makedirs(os.path.dirname(filename), exist_ok=True)  # create parent directory if it does not exist yet (ensure that the "output" directory is available). 
    
    with open(filename, 'a+') as f: # append generated word to existing file or create a new one if it doesn't exist.  
        f.write(f'{start_char} {word_length} {temperature}: {generated_word}\n') 
        
    print(f'\tGenerated word with parameters: start char={ord(start_char)}, length={word_length}, temperature={temperature}: {generated_word}.') # print info about generated word for debugging purposes.  
# main function that starts multiple processes and assigns them tasks to perform concurrently (generating words based on different seeds or start characters). 
def multiprocessing_func(start):
    with Pool(processes=num_processes) as pool: # create a pool of worker processes.  
        for _ in range(total_words//num_words):# loop through all combinations of initial character, word length and temperature settings to generate desired number of words per process (should be <= num_words). 
            char = start[random.randint(0, len(start)-1)] # select a random starting character from predefined options.  
            
            pool.apply_async(write_word, args=(char,))# assign task to worker process: generate single word with given parameters (initial character or seed, desired word length and chosen temperature setting). 
    
        print('\tAll tasks assigned! Waiting for results...') # indicate that all tasks have been assigned to worker processes.  
        
        while True: # wait until all worker processes have completed their tasks. 
            if pool.apply_async(write_word, args=(start[random.randint(0, len(start)-1)],)) == False: break# check if there are any more tasks to assign (if not then break out of infinite loop).  
        
    print('\tAll jobs completed!') # indicate that all worker processes have finished executing their assigned tasks. 

def main(): 
    t1 = time.time()  # start timer to measure execution time of the program.    
    
    for i in range(len(rngs)):# loop through each initial seed or starting character and assign it a unique process (or multiple processes if cpu_count is high enough).  
        multiprocessing_func([rngs[i]]) # start one or more worker processes to generate words based on given parameters. 
        
    print(f'All jobs completed! Total execution time: {time.time() - t1} seconds.') # indicate that all tasks have been completed and display total execution time in seconds.  
    
if __name__ == '__main__':  main()# run main function when program is executed directly (not imported as module). 