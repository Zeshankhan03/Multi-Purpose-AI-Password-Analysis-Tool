{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Generator and Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define the Generator class\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(vocab_size * max_length, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Flatten the input tensor\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "# Define the Discriminator class\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(vocab_size, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()  # Output is a probability of being a real password\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Password Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset class\n",
    "class PasswordDataset(Dataset):\n",
    "    def __init__(self, passwords, char2idx, max_length):\n",
    "        self.passwords = passwords\n",
    "        self.char2idx = char2idx\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.passwords)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        password = self.passwords[idx]\n",
    "        padded_password = self.pad_sequence([self.char2idx[char] for char in password])\n",
    "        return torch.tensor(padded_password, dtype=torch.long)\n",
    "    \n",
    "    def pad_sequence(self, seq):\n",
    "        padded = [self.char2idx[' ']] * self.max_length\n",
    "        padded[:len(seq)] = seq\n",
    "        return padded\n",
    "\n",
    "# Function to read passwords from a file\n",
    "def read_passwords(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        passwords = [line.strip() for line in file.readlines()]\n",
    "    return passwords\n",
    "\n",
    "# Load passwords from the file\n",
    "file_path = 'filtered_words_5_rockyou.txt'  # Replace with your file path\n",
    "passwords = read_passwords(file_path)\n",
    "\n",
    "# Create character mappings\n",
    "char2idx = {char: idx for idx, char in enumerate(sorted(set(''.join(passwords) + ' ')))}\n",
    "idx2char = {idx: char for char, idx in char2idx.items()}\n",
    "max_length = max(len(password) for password in passwords)\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = PasswordDataset(passwords, char2idx, max_length)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(sequence, vocab_size):\n",
    "    one_hot = torch.zeros(len(sequence), vocab_size)\n",
    "    for idx, char_idx in enumerate(sequence):\n",
    "        one_hot[idx, char_idx] = 1\n",
    "    return one_hot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m optimizer_D\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Discriminator loss on real passwords\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_passwords_one_hot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m d_loss_real \u001b[38;5;241m=\u001b[39m criterion(outputs, real_labels)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Generate fake passwords\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Osama Khalid\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Osama Khalid\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 43\u001b[0m, in \u001b[0;36mDiscriminator.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Osama Khalid\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Osama Khalid\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Osama Khalid\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Osama Khalid\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Osama Khalid\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Osama Khalid\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)"
     ]
    }
   ],
   "source": [
    "# Model parameters\n",
    "noise_dim = 100\n",
    "embedding_dim = 128\n",
    "hidden_dim = 256\n",
    "vocab_size = len(char2idx)\n",
    "\n",
    "# Instantiate the models\n",
    "G = Generator(noise_dim, vocab_size, hidden_dim).to(device)\n",
    "D = Discriminator(vocab_size, hidden_dim).to(device)\n",
    "\n",
    "# Loss and optimizers\n",
    "criterion = nn.BCELoss()\n",
    "optimizer_G = optim.Adam(G.parameters(), lr=0.0002)\n",
    "optimizer_D = optim.Adam(D.parameters(), lr=0.0002)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    for i, real_passwords in enumerate(dataloader):\n",
    "        real_passwords = real_passwords.to(device)\n",
    "        batch_size = real_passwords.size(0)\n",
    "\n",
    "        # One-hot encode real passwords\n",
    "        real_passwords_one_hot = torch.stack([one_hot_encode(pwd, vocab_size) for pwd in real_passwords])\n",
    "        \n",
    "        # Labels for real and fake passwords\n",
    "        real_labels = torch.ones(batch_size, 1).to(device)\n",
    "        fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "\n",
    "        # Train the discriminator\n",
    "        optimizer_D.zero_grad()\n",
    "        \n",
    "        # Discriminator loss on real passwords\n",
    "        outputs = D(real_passwords_one_hot.float())\n",
    "        d_loss_real = criterion(outputs, real_labels)\n",
    "        \n",
    "        # Generate fake passwords\n",
    "        z = torch.randn(batch_size, noise_dim).to(device)\n",
    "        fake_passwords = G(z)\n",
    "        \n",
    "        # Discriminator loss on fake passwords\n",
    "        outputs = D(fake_passwords.detach())\n",
    "        d_loss_fake = criterion(outputs, fake_labels)\n",
    "        \n",
    "        # Backprop and optimize\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        # Train the generator\n",
    "        optimizer_G.zero_grad()\n",
    "        \n",
    "        # Generator loss\n",
    "        outputs = D(fake_passwords)\n",
    "        g_loss = criterion(outputs, real_labels)\n",
    "        \n",
    "        # Backprop and optimize\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "        \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], d_loss: {d_loss.item():.4f}, g_loss: {g_loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Generate Passwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new passwords\n",
    "G.eval()\n",
    "generated_passwords = []\n",
    "with torch.no_grad():\n",
    "    for _ in range(100):\n",
    "        z = torch.randn(1, noise_dim).to(device)\n",
    "        fake_password = G(z)\n",
    "        password = ''.join([idx2char[idx] for idx in torch.argmax(fake_password, dim=1).cpu().numpy()])\n",
    "        generated_passwords.append(password.strip())\n",
    "\n",
    "# Print generated passwords\n",
    "for i, password in enumerate(generated_passwords):\n",
    "    print(f\"Password {i+1}: {password}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 126] The specified module could not be found. Error loading \"c:\\Users\\Osama Khalid\\.conda\\envs\\Testpytroch\\Lib\\site-packages\\torch\\lib\\fbgemm.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Osama Khalid\\.conda\\envs\\Testpytroch\\Lib\\site-packages\\torch\\__init__.py:148\u001b[0m\n\u001b[0;32m    146\u001b[0m                 err \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mWinError(ctypes\u001b[38;5;241m.\u001b[39mget_last_error())\n\u001b[0;32m    147\u001b[0m                 err\u001b[38;5;241m.\u001b[39mstrerror \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Error loading \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdll\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or one of its dependencies.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 148\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    150\u001b[0m     kernel32\u001b[38;5;241m.\u001b[39mSetErrorMode(prev_error_mode)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_preload_cuda_deps\u001b[39m(lib_folder, lib_name):\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 126] The specified module could not be found. Error loading \"c:\\Users\\Osama Khalid\\.conda\\envs\\Testpytroch\\Lib\\site-packages\\torch\\lib\\fbgemm.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Custom dataset class\n",
    "class PasswordDataset(Dataset):\n",
    "    def __init__(self, passwords, char2idx, max_length):\n",
    "        self.passwords = passwords\n",
    "        self.char2idx = char2idx\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.passwords)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        password = self.passwords[idx]\n",
    "        padded_password = self.pad_sequence([self.char2idx[char] for char in password])\n",
    "        return torch.tensor(padded_password, dtype=torch.long)\n",
    "    \n",
    "    def pad_sequence(self, seq):\n",
    "        padded = [self.char2idx[' ']] * self.max_length\n",
    "        padded[:len(seq)] = seq\n",
    "        return padded\n",
    "\n",
    "# One-hot encode function\n",
    "def one_hot_encode(sequence, vocab_size):\n",
    "    one_hot = torch.zeros(len(sequence), vocab_size)\n",
    "    for idx, char_idx in enumerate(sequence):\n",
    "        one_hot[idx, char_idx] = 1\n",
    "    return one_hot\n",
    "\n",
    "# Generator class\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_dim, vocab_size, max_length):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(noise_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, vocab_size * max_length),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def forward(self, z):\n",
    "        gen_output = self.model(z)\n",
    "        gen_output = gen_output.view(z.size(0), self.max_length, self.vocab_size)\n",
    "        return gen_output\n",
    "\n",
    "# Discriminator class\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, max_length):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(vocab_size * max_length, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Flatten the input tensor\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.model(x)\n",
    "\n",
    "# Read passwords from a file\n",
    "def read_passwords(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        passwords = [line.strip() for line in file.readlines()]\n",
    "    return passwords\n",
    "\n",
    "# Training parameters\n",
    "noise_dim = 100\n",
    "hidden_dim = 256\n",
    "max_length = 8\n",
    "batch_size = 128\n",
    "num_epochs = 16\n",
    "learning_rate = 0.0002\n",
    "\n",
    "# Load passwords from the file\n",
    "file_path = 'D:\\\\Osama Khalid\\\\Osama Khalid BSCY 7\\\\FYP Part 1\\\\AI Codes\\\\Filtered_Passwords_Dataset\\\\length_5\\\\filtered_words_00000001.txt'  # Replace with your file path\n",
    "print(\"Loading passwords...\")\n",
    "passwords = read_passwords(file_path)\n",
    "print(f\"Loaded {len(passwords)} passwords.\")\n",
    "\n",
    "# Create character mappings\n",
    "char2idx = {char: idx for idx, char in enumerate(sorted(set(''.join(passwords) + ' ')))}\n",
    "idx2char = {idx: char for char, idx in char2idx.items()}\n",
    "vocab_size = len(char2idx)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = PasswordDataset(passwords, char2idx, max_length)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "print(\"DataLoader created.\")\n",
    "\n",
    "# Initialize models\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "G = Generator(noise_dim, vocab_size, max_length).to(device)\n",
    "D = Discriminator(vocab_size, hidden_dim, max_length).to(device)\n",
    "\n",
    "# Loss and optimizers\n",
    "criterion = nn.BCELoss()\n",
    "optimizer_G = optim.Adam(G.parameters(), lr=learning_rate)\n",
    "optimizer_D = optim.Adam(D.parameters(), lr=learning_rate)\n",
    "\n",
    "print(\"Starting training...\")\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    running_d_loss = 0.0\n",
    "    running_g_loss = 0.0\n",
    "    with tqdm(dataloader, unit=\"batch\") as tepoch:\n",
    "        for real_passwords in tepoch:\n",
    "            tepoch.set_description(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "            real_passwords = real_passwords.to(device)\n",
    "            batch_size = real_passwords.size(0)\n",
    "\n",
    "            # One-hot encode real passwords\n",
    "            real_passwords_one_hot = torch.stack([one_hot_encode(pwd, vocab_size) for pwd in real_passwords]).to(device)\n",
    "            \n",
    "            # Labels for real and fake passwords\n",
    "            real_labels = torch.ones(batch_size, 1).to(device)\n",
    "            fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "\n",
    "            # Train the discriminator\n",
    "            optimizer_D.zero_grad()\n",
    "            \n",
    "            # Discriminator loss on real passwords\n",
    "            outputs = D(real_passwords_one_hot.float())\n",
    "            d_loss_real = criterion(outputs, real_labels)\n",
    "            \n",
    "            # Generate fake passwords\n",
    "            z = torch.randn(batch_size, noise_dim).to(device)\n",
    "            fake_passwords = G(z)\n",
    "            \n",
    "            # Discriminator loss on fake passwords\n",
    "            outputs = D(fake_passwords.detach())\n",
    "            d_loss_fake = criterion(outputs, fake_labels)\n",
    "            \n",
    "            # Backprop and optimize\n",
    "            d_loss = d_loss_real + d_loss_fake\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "            \n",
    "            # Train the generator\n",
    "            optimizer_G.zero_grad()\n",
    "            \n",
    "            # Generator loss\n",
    "            outputs = D(fake_passwords)\n",
    "            g_loss = criterion(outputs, real_labels)\n",
    "            \n",
    "            # Backprop and optimize\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            running_d_loss += d_loss.item()\n",
    "            running_g_loss += g_loss.item()\n",
    "\n",
    "            tepoch.set_postfix(d_loss=d_loss.item(), g_loss=g_loss.item())\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], d_loss: {running_d_loss/len(dataloader):.4f}, g_loss: {running_g_loss/len(dataloader):.4f}\")\n",
    "\n",
    "print(\"Training finished.\")\n",
    "\n",
    "# Save the models\n",
    "torch.save(G.state_dict(), 'password_generator.pth')\n",
    "torch.save(D.state_dict(), 'password_discriminator.pth')\n",
    "print(\"Models saved.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CPU CODE Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Custom dataset class\n",
    "class PasswordDataset(Dataset):\n",
    "    def __init__(self, passwords, char2idx, max_length):\n",
    "        self.passwords = passwords\n",
    "        self.char2idx = char2idx\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.passwords)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        password = self.passwords[idx]\n",
    "        padded_password = self.pad_sequence([self.char2idx[char] for char in password])\n",
    "        return torch.tensor(padded_password, dtype=torch.long)\n",
    "    \n",
    "    def pad_sequence(self, seq):\n",
    "        padded = [self.char2idx[' ']] * self.max_length\n",
    "        padded[:len(seq)] = seq\n",
    "        return padded\n",
    "\n",
    "# One-hot encode function\n",
    "def one_hot_encode(sequence, vocab_size):\n",
    "    one_hot = torch.zeros(len(sequence), vocab_size)\n",
    "    for idx, char_idx in enumerate(sequence):\n",
    "        one_hot[idx, char_idx] = 1\n",
    "    return one_hot\n",
    "\n",
    "# Generator class\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_dim, vocab_size, max_length):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(noise_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, vocab_size * max_length),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def forward(self, z):\n",
    "        gen_output = self.model(z)\n",
    "        gen_output = gen_output.view(z.size(0), self.max_length, self.vocab_size)\n",
    "        return gen_output\n",
    "\n",
    "# Discriminator class\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, max_length):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(vocab_size * max_length, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Flatten the input tensor\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.model(x)\n",
    "\n",
    "# Read passwords from a file\n",
    "def read_passwords(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        passwords = [line.strip() for line in file.readlines()]\n",
    "    return passwords\n",
    "\n",
    "# Training parameters\n",
    "noise_dim = 100\n",
    "hidden_dim = 256\n",
    "max_length = 8\n",
    "batch_size = 64\n",
    "num_epochs = 100\n",
    "learning_rate = 0.0002\n",
    "\n",
    "# Load passwords from the file\n",
    "file_path = 'filtered_words_5_rockyou.txt'  # Replace with your file path\n",
    "print(\"Loading passwords...\")\n",
    "passwords = read_passwords(file_path)\n",
    "print(f\"Loaded {len(passwords)} passwords.\")\n",
    "\n",
    "# Create character mappings\n",
    "char2idx = {char: idx for idx, char in enumerate(sorted(set(''.join(passwords) + ' ')))}\n",
    "idx2char = {idx: char for char, idx in char2idx.items()}\n",
    "vocab_size = len(char2idx)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = PasswordDataset(passwords, char2idx, max_length)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "print(\"DataLoader created.\")\n",
    "\n",
    "# Initialize models\n",
    "device = torch.device('cpu')\n",
    "G = Generator(noise_dim, vocab_size, max_length).to(device)\n",
    "D = Discriminator(vocab_size, hidden_dim, max_length).to(device)\n",
    "\n",
    "# Loss and optimizers\n",
    "criterion = nn.BCELoss()\n",
    "optimizer_G = optim.Adam(G.parameters(), lr=learning_rate)\n",
    "optimizer_D = optim.Adam(D.parameters(), lr=learning_rate)\n",
    "\n",
    "print(\"Starting training...\")\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    running_d_loss = 0.0\n",
    "    running_g_loss = 0.0\n",
    "    with tqdm(dataloader, unit=\"batch\") as tepoch:\n",
    "        for real_passwords in tepoch:\n",
    "            tepoch.set_description(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "            real_passwords = real_passwords.to(device)\n",
    "            batch_size = real_passwords.size(0)\n",
    "\n",
    "            # One-hot encode real passwords\n",
    "            real_passwords_one_hot = torch.stack([one_hot_encode(pwd, vocab_size) for pwd in real_passwords]).to(device)\n",
    "            \n",
    "            # Labels for real and fake passwords\n",
    "            real_labels = torch.ones(batch_size, 1).to(device)\n",
    "            fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "\n",
    "            # Train the discriminator\n",
    "            optimizer_D.zero_grad()\n",
    "            \n",
    "            # Discriminator loss on real passwords\n",
    "            outputs = D(real_passwords_one_hot.float())\n",
    "            d_loss_real = criterion(outputs, real_labels)\n",
    "            \n",
    "            # Generate fake passwords\n",
    "            z = torch.randn(batch_size, noise_dim).to(device)\n",
    "            fake_passwords = G(z)\n",
    "            \n",
    "            # Discriminator loss on fake passwords\n",
    "            outputs = D(fake_passwords.detach())\n",
    "            d_loss_fake = criterion(outputs, fake_labels)\n",
    "            \n",
    "            # Backprop and optimize\n",
    "            d_loss = d_loss_real + d_loss_fake\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "            \n",
    "            # Train the generator\n",
    "            optimizer_G.zero_grad()\n",
    "            \n",
    "            # Generator loss\n",
    "            outputs = D(fake_passwords)\n",
    "            g_loss = criterion(outputs, real_labels)\n",
    "            \n",
    "            # Backprop and optimize\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            running_d_loss += d_loss.item()\n",
    "            running_g_loss += g_loss.item()\n",
    "\n",
    "            tepoch.set_postfix(d_loss=d_loss.item(), g_loss=g_loss.item())\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], d_loss: {running_d_loss/len(dataloader):.4f}, g_loss: {running_g_loss/len(dataloader):.4f}\")\n",
    "\n",
    "print(\"Training finished.\")\n",
    "\n",
    "# Save the models\n",
    "torch.save(G.state_dict(), 'password_generator_cpu.pth')\n",
    "torch.save(D.state_dict(), 'password_discriminator_cpu.pth')\n",
    "print(\"Models saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 43\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(x)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Load the saved generator and discriminator models\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m generator \u001b[38;5;241m=\u001b[39m Generator(noise_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, vocab_size\u001b[38;5;241m=\u001b[39m\u001b[43mvocab_size\u001b[49m, max_length\u001b[38;5;241m=\u001b[39mmax_length)\n\u001b[0;32m     44\u001b[0m generator\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpassword_generator.pth\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     45\u001b[0m generator\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vocab_size' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# Define the Generator class\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_dim, vocab_size, max_length):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(noise_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, vocab_size * max_length),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def forward(self, z):\n",
    "        gen_output = self.model(z)\n",
    "        gen_output = gen_output.view(z.size(0), self.max_length, self.vocab_size)\n",
    "        return gen_output\n",
    "\n",
    "# Define the Discriminator class\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, max_length):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(vocab_size * max_length, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Flatten the input tensor\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.model(x)\n",
    "\n",
    "# Load the saved generator and discriminator models\n",
    "generator = Generator(noise_dim=100, vocab_size=vocab_size, max_length=max_length)\n",
    "generator.load_state_dict(torch.load('password_generator.pth'))\n",
    "generator.eval()\n",
    "\n",
    "discriminator = Discriminator(vocab_size=vocab_size, hidden_dim=hidden_dim, max_length=max_length)\n",
    "discriminator.load_state_dict(torch.load('password_discriminator.pth'))\n",
    "discriminator.eval()\n",
    "\n",
    "# Generate 100 passwords using the generator\n",
    "num_passwords = 100000\n",
    "noise = torch.randn(num_passwords, noise_dim)\n",
    "with torch.no_grad():\n",
    "    generated_passwords = generator(noise).argmax(dim=-1)\n",
    "\n",
    "# Decode the generated passwords\n",
    "generated_passwords_decoded = []\n",
    "for password in generated_passwords:\n",
    "    password_decoded = ''.join([idx2char[idx.item()] for idx in password if idx.item() != char2idx[' ']])\n",
    "    generated_passwords_decoded.append(password_decoded)\n",
    "\n",
    "# Print the generated passwords\n",
    "for i, password in enumerate(generated_passwords_decoded, start=1):\n",
    "    print(f\"Password {i}: {password}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
